{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pymongo (Connecting MongoDB with Python) as well as other ML libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, concatenate\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to Database\n",
    "client = MongoClient(\"mongodb+srv://webServer:hkSEd64DH1wujNPD@slugmetercluster.de0aesc.mongodb.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the database\n",
    "db = client.SlugMeterTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the timestamp data\n",
    "TimeStamps = db.ML\n",
    "stamps = TimeStamps.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing DB as dataframe\n",
    "df = pd.DataFrame(list(TimeStamps.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unecessary columns (Only care about the timestamps and dates)\n",
    "df = df.drop(columns = \"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Timestamps into Ints\n",
    "index = 0\n",
    "\n",
    "for i in df['time']:\n",
    "    info = i[:-9]\n",
    "    new_info = info[:2] + \"\" + info[-2:]\n",
    "    new_info = int(new_info)\n",
    "    df.loc[index, 'time'] = new_info\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a seperate column for all times in terms of hours\n",
    "index = 0\n",
    "for i in df['time']:\n",
    "    new = i//100\n",
    "    df.loc[index, 'hours'] = new\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a seperate column for the day of the week\n",
    "#Encoding: Mon:0, Tues:1, Wed:2, Thur:3, Fri:4, Sat:5, Sun:6\n",
    "index = 0\n",
    "for full_date in df['date']:\n",
    "    df.loc[index, 'Day'] = full_date.weekday()\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('gym_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a seperate column for the day of the week\n",
    "#Encoding: Mon:0, Tues:1, Wed:2, Thur:3, Fri:4, Sat:5, Sun:6\n",
    "index = 0\n",
    "for num_peep in data['number_people']:\n",
    "    df.loc[index, 'Num_of_people'] = num_peep\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a seperate column for holidays\n",
    "#If it is a holiday, zero-out the Num_of_people\n",
    "index = 0\n",
    "for actual_date in df['date']:\n",
    "    if(actual_date.month == 11 and actual_date.day == 10):\n",
    "        df.iloc[index, 4] = 0\n",
    "        df.loc[index, 'isHoliday'] = 1\n",
    "    elif(actual_date.month == 11 and (actual_date.day == 23 or actual_date.day == 24)):\n",
    "        df.iloc[index, 4] = 0\n",
    "        df.loc[index, 'isHoliday'] = 1\n",
    "    elif (actual_date.month == 12 and any(actual_date.day == i for i in range(25, 32))):\n",
    "        df.iloc[index, 4] = 0\n",
    "        df.loc[index, 'isHoliday'] = 1\n",
    "    elif(actual_date.month == 1 and actual_date.day == 1):\n",
    "        df.iloc[index, 4] = 0\n",
    "        df.loc[index, 'isHoliday'] = 1\n",
    "    else:\n",
    "        df.loc[index, 'isHoliday'] = 0\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any NaN values \n",
    "df = df.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Exploration**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to plot interactive plots using Plotly (Reference: https://medium.com/mlearning-ai/forecasting-timeseries-using-machine-learning-deep-learning-446eccc6eb6d)\n",
    "# def plotl(df, x, y, title):\n",
    "#     fig = px.line(df, x=x, y=y, title=title)\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotl(df, 'date', df['Num_of_people'], 'Number of people (Year)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People seemed to stop going to the gym as the years passed. Seems like a steady decline, could be an issue with how the data was obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a bar chart to show num of people during each day\n",
    "# plt.bar(df['Day'], df['Num_of_people'], width = 0.6)\n",
    "# plt.xlabel('Day of Week (Mon:0, Tues:1, Wed:2, Thur:3, Fri:4, Sat:5, Sun:6)')\n",
    "# plt.ylabel('Num of People')\n",
    "# plt.title('Number of people (Day)')\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of people seems to be evenly disbursed throughout the days of the week. There is more activity towards the start/middle of the week and it falls off during the weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a bar chart to show num of people during each hour\n",
    "# plt.bar(df['hours'], df['Num_of_people'], width = 0.6)\n",
    "# plt.xlabel('Hour of Day')\n",
    "# plt.ylabel('Num of People')\n",
    "# plt.title('Number of people (Hour)')\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, people aren't going to the early in the morning, definitely a spike around noon and high activities throughout the day."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Working LSTM Multivariate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create a subset of original dataframe to train model based on hours/day\n",
    "# df_hour_day = df.loc[:, ['hours', 'Day', 'Num_of_people']]\n",
    "# df_hour_day = df_hour_day.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split data\n",
    "# train_df,test_df = df_hour_day[0:12437], df_hour_day[12437:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Normalize the dataset\n",
    "# train = train_df\n",
    "# scalers={}\n",
    "# for i in train_df.columns:\n",
    "#     scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "#     s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
    "#     s_s=np.reshape(s_s,len(s_s))\n",
    "#     scalers['scaler_'+ i] = scaler\n",
    "#     train[i]=s_s\n",
    "# test = test_df\n",
    "# for i in train_df.columns:\n",
    "#     scaler = scalers['scaler_'+i]\n",
    "#     s_s = scaler.transform(test[i].values.reshape(-1,1))\n",
    "#     s_s=np.reshape(s_s,len(s_s))\n",
    "#     scalers['scaler_'+i] = scaler\n",
    "#     test[i]=s_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #function that will use a sliding window approach to transform our series into samples of input past observations and output future observations\n",
    "# def split_series(series, n_past, n_future):\n",
    "#   #\n",
    "#   # n_past ==> no of past observations\n",
    "#   #\n",
    "#   # n_future ==> no of future observations \n",
    "#   #\n",
    "#   X, y = list(), list()\n",
    "#   for window_start in range(len(series)):\n",
    "#     past_end = window_start + n_past\n",
    "#     future_end = past_end + n_future\n",
    "#     if future_end > len(series):\n",
    "#       break\n",
    "#     # slicing the past and future parts of the window\n",
    "#     past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "#     X.append(past)\n",
    "#     y.append(future)\n",
    "#   return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "# n_past = 10\n",
    "# n_future = 5 \n",
    "# n_features = 3\n",
    "\n",
    "# X_train, y_train = split_series(train.values,n_past, n_future)\n",
    "# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1],n_features))\n",
    "# y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], n_features))\n",
    "# X_test, y_test = split_series(test.values,n_past, n_future)\n",
    "# X_test = X_test.reshape((X_test.shape[0], X_test.shape[1],n_features))\n",
    "# y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Build the model\n",
    "# # n_features ==> no of features at each timestep in the data.\n",
    "# encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
    "# encoder_l1 = tf.keras.layers.LSTM(100, return_state=True)\n",
    "# encoder_outputs1 = encoder_l1(encoder_inputs)\n",
    "\n",
    "# encoder_states1 = encoder_outputs1[1:]\n",
    "\n",
    "# #\n",
    "# decoder_inputs = tf.keras.layers.RepeatVector(n_future)(encoder_outputs1[0])\n",
    "\n",
    "# #\n",
    "# decoder_l1 = tf.keras.layers.LSTM(100, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
    "# decoder_outputs1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l1)\n",
    "\n",
    "# #\n",
    "# model_e1d1 = tf.keras.models.Model(encoder_inputs,decoder_outputs1)\n",
    "\n",
    "# #\n",
    "# model_e1d1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit the model\n",
    "# reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
    "# model_e1d1.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "# history_e1d1=model_e1d1.fit(X_train,y_train,epochs=25,validation_data=(X_test,y_test),batch_size=32,verbose=0,callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Prediction on test samples\n",
    "# pred_e1d1=model_e1d1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Inverse Scaling of the predicted values\n",
    "# for index,i in enumerate(train_df.columns):\n",
    "#     scaler = scalers['scaler_'+i]\n",
    "#     pred_e1d1[:,:,index]=scaler.inverse_transform(pred_e1d1[:,:,index])\n",
    "#     y_train[:,:,index]=scaler.inverse_transform(y_train[:,:,index])\n",
    "#     y_test[:,:,index]=scaler.inverse_transform(y_test[:,:,index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# for index,i in enumerate(train_df.columns):\n",
    "#   print(i)\n",
    "#   for j in range(1,6):\n",
    "#     print(\"Day \",j,\":\")\n",
    "#     print(\"MAE : \",mean_absolute_error(y_test[:,j-1,index],pred_e1d1[:,j-1,index]),end=\", \")\n",
    "#   print()\n",
    "#   print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
